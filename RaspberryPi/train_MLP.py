import os
import random
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, Subset
import numpy as np

# -------------------------------------------------------------------------
# MLP Architecture Definition
# -------------------------------------------------------------------------
class MLP(nn.Module):
    def __init__(self, num_classes=10):
        super(MLP, self).__init__()
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(28*28, 400),
            nn.ReLU(),
            nn.Linear(400, 200),
            nn.ReLU(),
            nn.Linear(200, 100),
            nn.ReLU(),
            nn.Linear(100, num_classes)
        )

    def forward(self, x):
        return self.classifier(x)

# -------------------------------------------------------------------------
# Transforms + Dataset
# -------------------------------------------------------------------------
transform = transforms.Compose([
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

full_dataset = datasets.ImageFolder(root='training_digits', transform=transform)

num_classes = 10
targets = [sample[1] for sample in full_dataset.samples]
train_indices = []
test_indices = []

random.seed(42)
for class_idx in range(num_classes):
    idxs_class = [i for i, t in enumerate(targets) if t == class_idx]
    random.shuffle(idxs_class)
    train_indices.extend(idxs_class[:26])
    test_indices.extend(idxs_class[26:30])

train_dataset = Subset(full_dataset, train_indices)
test_dataset  = Subset(full_dataset, test_indices)

train_loader = DataLoader(train_dataset, batch_size=12, shuffle=True)
test_loader  = DataLoader(test_dataset, batch_size=12, shuffle=False)

# -------------------------------------------------------------------------
# Instantiate MLP
# -------------------------------------------------------------------------
model = MLP(num_classes=num_classes)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# -------------------------------------------------------------------------
# Training
# -------------------------------------------------------------------------
num_epochs = 20
print("Training MLP...")
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    for images, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(images)  # images => (N,1,28,28)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    avg_loss = running_loss / len(train_loader)
    print(f"Epoch {epoch+1}, Average Loss: {avg_loss:.4f}")

# Save PyTorch model to disk
torch.save(model.state_dict(), "mlp_digits_model.pth")

# -------------------------------------------------------------------------
# Evaluation
# -------------------------------------------------------------------------
model.eval()
correct = 0
total = 0
with torch.no_grad():
    for images, labels in test_loader:
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

accuracy = 100.0 * correct / total
print(f"\nMLP Test Accuracy: {accuracy:.2f}%")

# -------------------------------------------------------------------------
# Export Weights => mlp_weights.h
# -------------------------------------------------------------------------
# Reload the model state (just to be consistent with your example)
model.load_state_dict(torch.load("mlp_digits_model.pth",weights_only=True))
model.eval()

# We'll gather the nn.Linear layers from the classifier
fc_layers = []
for layer in model.classifier:
    if isinstance(layer, nn.Linear):
        fc_layers.append(layer)

with open("mlp_weights.h", "w") as f:
    f.write("// mlp_weights.h : Exported weights for MLP\n")
    f.write("// Generated by train_MLP.py\n\n")

    # We'll define how many fully connected layers we have
    f.write(f"#define MLP_NUM_LAYERS {len(fc_layers)}\n\n")

    # We'll define macros for activation types, as you did with CNN
    f.write("#define ACT_NONE 0\n")
    f.write("#define ACT_RELU 1\n\n")

    # Define a struct for a fully connected layer
    f.write("typedef struct {\n")
    f.write("    int in_features;\n")
    f.write("    int out_features;\n")
    f.write("    int activation_type;\n")
    f.write("    const float *weight;\n")
    f.write("    const float *bias;\n")
    f.write("} FCLayerDef;\n\n")

    # 1) Export each FC layer's weight & bias arrays
    for idx, fc in enumerate(fc_layers):
        W = fc.weight.detach().cpu().numpy()  # shape [out_features, in_features]
        b = fc.bias.detach().cpu().numpy()    # shape [out_features]

        weight_name = f"FC{idx}_WEIGHT"
        bias_name   = f"FC{idx}_BIAS"

        # Flatten row by row: [out_f, in_f]
        f.write(f"static const float {weight_name}[] = {{\n")
        out_f, in_f = W.shape
        for of_i in range(out_f):
            row_vals = ", ".join(str(x) for x in W[of_i])
            f.write(f"    {row_vals},\n")
        f.write("};\n\n")

        # Bias array
        f.write(f"static const float {bias_name}[] = {{ ")
        f.write(", ".join(str(x) for x in b))
        f.write("};\n\n")

    # 2) Now define an array of FCLayerDef structures
    #    We'll assume ReLU for hidden layers and ACT_NONE for the last
    f.write("static FCLayerDef MLP_LAYERS[MLP_NUM_LAYERS] = {\n")
    for idx, fc in enumerate(fc_layers):
        W = fc.weight.detach().cpu().numpy()
        out_f, in_f = W.shape

        # We'll do ReLU for each hidden layer, but none for the final layer
        if idx < len(fc_layers) - 1:
            act_type = "ACT_RELU"
        else:
            act_type = "ACT_NONE"

        weight_name = f"FC{idx}_WEIGHT"
        bias_name   = f"FC{idx}_BIAS"

        f.write(f"    {{ {in_f}, {out_f}, {act_type}, {weight_name}, {bias_name} }},\n")
    f.write("};\n\n")

print("MLP export completed. File generated: mlp_weights.h")
